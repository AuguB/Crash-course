{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install https://github.com/amarquand/PCNtoolkit/archive/refs/tags/v1.alpha.zip\n",
    "# %pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_hbr import NormHBR\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "from pcntoolkit.regression_model.hbr.likelihood import NormalLikelihood\n",
    "from pcntoolkit.regression_model.hbr.prior import make_prior\n",
    "from pcntoolkit.util.runner import Runner\n",
    "from pcntoolkit.regression_model.blr.blr_conf import BLRConf\n",
    "from pcntoolkit.normative_model.norm_blr import NormBLR\n",
    "from pcntoolkit.regression_model.hbr.likelihood import SHASHbLikelihood\n",
    "resources_dir = \"resources\"\n",
    "abs_path = os.path.abspath(resources_dir)\n",
    "data_dir = os.path.join(abs_path, \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook for the first time, you need to download the dataset from github.\n",
    "# If you have already downloaded the dataset, you can comment out the following line\n",
    "\n",
    "pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/predictive-clinical-neuroscience/PCNtoolkit-demo/refs/heads/main/data/fcon1000.csv\"\n",
    ").to_csv(os.path.join(data_dir, \"fcon1000.csv\"), index=False)\n",
    "data = pd.read_csv(os.path.join(data_dir, \"fcon1000.csv\"))\n",
    "covariates = [\"age\"]\n",
    "batch_effects = [\"sex\", \"site\"]\n",
    "response_vars = \"lh_G_temp_sup-G_T_transv_thickness,lh_G_temp_sup-Lateral_thickness,lh_G_temp_sup-Plan_polar_thickness\".split(',')\n",
    "# response_vars = \"lh_G&S_frontomargin_thickness,lh_G&S_occipital_inf_thickness,lh_G&S_paracentral_thickness,lh_G&S_subcentral_thickness,lh_G&S_transv_frontopol_thickness,lh_G&S_cingul-Ant_thickness,lh_G&S_cingul-Mid-Ant_thickness,lh_G&S_cingul-Mid-Post_thickness,lh_G_cingul-Post-dorsal_thickness,lh_G_cingul-Post-ventral_thickness,lh_G_cuneus_thickness,lh_G_front_inf-Opercular_thickness,lh_G_front_inf-Orbital_thickness,lh_G_front_inf-Triangul_thickness,lh_G_front_middle_thickness,lh_G_front_sup_thickness,lh_G_Ins_lg&S_cent_ins_thickness,lh_G_insular_short_thickness,lh_G_occipital_middle_thickness,lh_G_occipital_sup_thickness,lh_G_oc-temp_lat-fusifor_thickness,lh_G_oc-temp_med-Lingual_thickness,lh_G_oc-temp_med-Parahip_thickness,lh_G_orbital_thickness,lh_G_pariet_inf-Angular_thickness\".split(\",\")\n",
    "norm_data = NormData.from_dataframe(\n",
    "    name=\"full\",\n",
    "    dataframe=data,\n",
    "    covariates=covariates,\n",
    "    batch_effects=batch_effects,\n",
    "    response_vars=response_vars,\n",
    ")\n",
    "\n",
    "# Leave two sites out for doing transfer and extend later\n",
    "transfer_sites = [\"Milwaukee_b\", \"Oulu\"]\n",
    "transfer_data, fit_data = norm_data.split_batch_effects(\n",
    "    {\"site\": transfer_sites}, names=(\"transfer\", \"fit\")\n",
    ")\n",
    "\n",
    "# Split into train and test sets\n",
    "train, test = fit_data.train_test_split()\n",
    "transfer_train, transfer_test = transfer_data.train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 754047 - Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Create a NormConf object\n",
    "save_dir = os.path.join(abs_path, \"save_dir\")\n",
    "norm_conf = NormConf(\n",
    "    savemodel=True,\n",
    "    saveresults=True,\n",
    "    save_dir=save_dir,\n",
    "    inscaler=\"standardize\",\n",
    "    outscaler=\"standardize\",\n",
    "    basis_function=\"bspline\",\n",
    "    basis_function_kwargs={\"order\": 3, \"nknots\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = make_prior(\n",
    "    linear=True,\n",
    "    slope=make_prior(dist_name=\"Normal\", dist_params=(0.0, 10.0)),\n",
    "    intercept=make_prior(\n",
    "        random=True,\n",
    "        mu=make_prior(dist_name=\"Normal\", dist_params=(0.0, 1.0)),\n",
    "        sigma=make_prior(dist_name=\"HalfCauchy\", dist_params=(0.5,)),\n",
    "    ),\n",
    ")\n",
    "sigma = make_prior(\n",
    "    linear=True,\n",
    "    slope=make_prior(dist_name=\"Normal\", dist_params=(0.0, 10.0)),\n",
    "    intercept=make_prior(dist_name=\"Normal\", dist_params=(1.0, 1.0)),\n",
    "    mapping=\"softplus\",\n",
    "    mapping_params=(0.0, 3.0),\n",
    ")\n",
    "# epsilon = make_prior(\n",
    "#     dist_name=\"Normal\",\n",
    "#     dist_params=(0.0, 1.0),\n",
    "# )\n",
    "# delta = make_prior(\n",
    "#     dist_name=\"Normal\",\n",
    "#     dist_params=(1.0, 2.0),\n",
    "#     mapping=\"softplus\",\n",
    "#     mapping_params=(0.0, 3.0, 0.6),\n",
    "# )\n",
    "\n",
    "# Configure the HBRConf object\n",
    "hbr_conf = HBRConf(\n",
    "    draws=1500,\n",
    "    tune=500,\n",
    "    chains=4,\n",
    "    pymc_cores=16,\n",
    "    likelihood=NormalLikelihood(mu, sigma),\n",
    "    nuts_sampler=\"nutpie\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hbr_model = NormHBR(norm_conf=norm_conf, reg_conf=hbr_conf)\n",
    "sandbox_dir = os.path.join(resources_dir, \"runner_dir\")\n",
    "os.makedirs(sandbox_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 754047 - \n",
      "Job Status Monitor:\n",
      "--------------------------------------------\n",
      "Job ID     Name     State     Time     Nodes\n",
      "--------------------------------------------\n",
      "\n",
      "Process: 754047 - 46816177   normative_job_0 RUNNING   0:30     dccn-c089\n",
      "Process: 754047 - 46816178   normative_job_1 RUNNING   0:30     dccn-c089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m      2\u001b[0m     cross_validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     cv_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     temp_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sandbox_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_hbr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/3022000.05/projects/stijdboe/envs/crash-course/lib/python3.12/site-packages/pcntoolkit/util/runner.py:134\u001b[0m, in \u001b[0;36mRunner.fit_predict\u001b[0;34m(self, model, fit_data, predict_data, save_dir, observe)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observe:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_observer \u001b[38;5;241m=\u001b[39m JobObserver(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_jobs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_type)\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_observer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_jobs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinished_jobs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailed_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_jobs_status()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/project/3022000.05/projects/stijdboe/envs/crash-course/lib/python3.12/site-packages/pcntoolkit/util/job_observer.py:119\u001b[0m, in \u001b[0;36mJobObserver.wait_for_jobs\u001b[0;34m(self, check_interval)\u001b[0m\n\u001b[1;32m    116\u001b[0m     in_notebook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_notebook:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_jobs_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_jobs_terminal(check_interval)\n",
      "File \u001b[0;32m/project/3022000.05/projects/stijdboe/envs/crash-course/lib/python3.12/site-packages/pcntoolkit/util/job_observer.py:155\u001b[0m, in \u001b[0;36mJobObserver.wait_for_jobs_notebook\u001b[0;34m(self, check_interval)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_job_ids[job_name]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_job_ids:\n\u001b[0;32m--> 155\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m Output\u001b[38;5;241m.\u001b[39mprint(Messages\u001b[38;5;241m.\u001b[39mALL_JOBS_COMPLETED)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner = Runner(\n",
    "    cross_validate=True,\n",
    "    cv_folds=3,\n",
    "    time_limit=\"00:10:00\",\n",
    "    job_type=\"slurm\",\n",
    "    log_dir=os.path.join(sandbox_dir, \"log_dir\"),\n",
    "    temp_dir=os.path.join(sandbox_dir, \"temp_dir\"),\n",
    ")\n",
    "\n",
    "runner.fit_predict(new_hbr_model, train, test, observe=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
